{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99a13798-137c-4bc5-a73b-235834b9c7b5",
   "metadata": {},
   "source": [
    "# Save & Load Models\n",
    "\n",
    "Based on **Patric Loeber** video: https://www.youtube.com/watch?v=c36lUUr864M&t=15434s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f5df08-c2dc-4aa6-b835-2fe25f92c05e",
   "metadata": {},
   "source": [
    "There are the only 3 different methods:\n",
    "+ torch.save(arg, PATH)\n",
    "+ torch.load(PATH)\n",
    "+ model.load_state_dict(arg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5118ef5e-9a41-4702-8c7a-afb0609788d8",
   "metadata": {},
   "source": [
    "torch.save can use tensors, models or any dictionary as parameter for saving, makes use of python pickle module to serialize the objects and saves them so the result is serialized and not human readable.\n",
    "\n",
    "For saving our model we have two options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c007653-cd1c-4464-815b-f2aab929bb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#### COMPLETE MODEL ####\n",
    "torch.save(model, PATH)\n",
    "\n",
    "# model class must be defined somewhere\n",
    "model = torch.load(PATH)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8911253-1be1-4e07-9a61-2eb34c8dbd5b",
   "metadata": {},
   "source": [
    "### Lazy method\n",
    "\n",
    "We are just calling torch.save on our model. We have to specify the path or the file name. Later when we want to load our model we just set up our model by typing model=torch.load(PATH). Then we also want to set our model to evaluation method.\n",
    "\n",
    "Disadvantage of this approach is that the serialized data is bound to the specific classes and the exact directory structure that is used when the model is saved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854ce538-fd39-40e0-a02c-1b9598ba6e54",
   "metadata": {},
   "source": [
    "### Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90fd7795-21de-485d-889b-0ccbce24daee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n_input_features):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = nn.Linear(n_input_features, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y_pred = torch.sigmoid(self.linear(x))\n",
    "        return y_pred\n",
    "    \n",
    "model = Model(n_input_features=6)\n",
    "# train our model...\n",
    "\n",
    "# lazy method\n",
    "\n",
    "FILE = \"model.pth\" # it is common to use .pth -> PyTorch\n",
    "torch.save(model, FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa68f372-fc55-4440-bcd2-0958af91379e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.4053, -0.0697,  0.2480, -0.0789,  0.2890, -0.2310]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3592], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(FILE)\n",
    "model.eval()\n",
    "\n",
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8563d2b8-b704-436d-8b0e-942b873afc6a",
   "metadata": {},
   "source": [
    "### Recomended way of saving our model\n",
    "\n",
    "If we just want to save our train model and use it later for inference then it is enough to only save the parameters. We can save any dictionary with torch.save so we can save the parameters by calling **torch.save** with **model.state_dict()** which hold the parameters and then the **PATH**. Later when we want to load our model again first we have to create the model object and then we call the **model.load_state_dict()** and then inside it we call **torch.load(PATH)**. We have to be careful since load state dict doesn't take only a path but loaded dictionary here. Then again we set our model to evaluation mode. This is the preferred way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a21582-d385-49fc-8bc7-e7602c9a51ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### STATE DICT ####\n",
    "torch.save(model.state_dict(), PATH)\n",
    "\n",
    "# model must be crated agai with parameters\n",
    "model = Model(*args, **kwargs)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753f6c28-7b02-4872-aed3-834359b4093c",
   "metadata": {},
   "source": [
    "### Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d8b06bb-25a3-4e16-a2b0-49749c09fe3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0180, -0.1813, -0.0905,  0.2800, -0.0344,  0.0454]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1517], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n_input_features):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = nn.Linear(n_input_features, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y_pred = torch.sigmoid(self.linear(x))\n",
    "        return y_pred\n",
    "    \n",
    "model = Model(n_input_features=6)\n",
    "# train our model...\n",
    "\n",
    "for param in model.parameters():\n",
    "    print(param)\n",
    "# prefered method\n",
    "\n",
    "FILE = \"model.pth\" # it is common to use .pth -> PyTorch\n",
    "torch.save(model.state_dict(), FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ba427fd-8c83-48b0-8531-5e7718bc45c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0180, -0.1813, -0.0905,  0.2800, -0.0344,  0.0454]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1517], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "loaded_model = Model(n_input_features=6)\n",
    "loaded_model.load_state_dict(torch.load(FILE))\n",
    "loaded_model.eval()\n",
    "\n",
    "for param in loaded_model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfe7860-0194-4dc6-bd03-7691fac0c845",
   "metadata": {},
   "source": [
    "## Saving a whole checkpoint during training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae51cd3-8c42-4861-9f5b-e7ccd94963f0",
   "metadata": {},
   "source": [
    "Let's say we want to stop somewhere at some point during training and save a checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c06eb61-e9c2-4e1b-8aa2-554a0237371e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'state': {}, 'param_groups': [{'lr': 0.01, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'params': [0, 1]}]}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n_input_features):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = nn.Linear(n_input_features, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y_pred = torch.sigmoid(self.linear(x))\n",
    "        return y_pred\n",
    "    \n",
    "model = Model(n_input_features=6)\n",
    "# train our model...\n",
    "\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "print(optimizer.state_dict())\n",
    "\n",
    "checkpoint = {\n",
    "    \"epoch\": 90,\n",
    "    \"model_state\": model.state_dict(),\n",
    "    \"optim_state\": optimizer.state_dict()\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, \"checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13afd388-921a-4ada-9989-d4bb1632f762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'state': {}, 'param_groups': [{'lr': 0.01, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'params': [0, 1]}]}\n"
     ]
    }
   ],
   "source": [
    "loaded_checkpoint = torch.load(\"checkpoint.pth\")\n",
    "epoch = loaded_checkpoint[\"epoch\"]\n",
    "\n",
    "model = Model(n_input_features=6)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0)\n",
    "\n",
    "model.load_state_dict(checkpoint[\"model_state\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optim_state\"])\n",
    "\n",
    "print(optimizer.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795b23f5-6cb8-4f11-8abd-45c069beece0",
   "metadata": {},
   "source": [
    "## Using GPU during training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e08c37-0ac1-41f9-a7da-1853da3f4332",
   "metadata": {},
   "source": [
    "### Save on GPU, Load on CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d925dd-66b2-4fe9-9cc0-e2ae9f88692b",
   "metadata": {},
   "source": [
    "If we save our model on the GPU and then later we want to load on the cpu then we have to do it this way. Let' say somewhere during our training we set up our cuda device and we send our odel to the device and then we save it by using the state dict. Then we want to load it to the CPU. So we have our cpu and then we create our model again and we call **model.load_state_dict()** with **torch.load(PATH, map_location=device)** inside. We have to specify the map location, here we give it the cpu device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314b1ad5-3577-47f7-b3b3-ddeea716104e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Save on GPU, Load on CPU\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "torch.save(model_state_dict(), PATH)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = Model(*args, **kwargs)\n",
    "model.load_state_dict(torch.load(PATH, map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c386b0-87c1-43ef-9554-3bdbf1a42c93",
   "metadata": {},
   "source": [
    "### Save on GPU, Load on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ec916c-2a65-4a6a-9cb8-b2daf4f81eee",
   "metadata": {},
   "source": [
    "If we want to do both save and load on the GPU. We just send our model to the cuda device and save it. Then we just set up our model and use **load_state_dict()** method with **torch.load(PATH)** inside and then we send our model to the cuda device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f461627b-cc3c-4e65-8973-ddc1d55f59d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Save on GPU, Load on GPU\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "torch.save(model_state_dict(), PATH)\n",
    "\n",
    "model = Model(*args, **kwargs)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b61ba03-ac3e-41e8-8cc3-65bf74f2267f",
   "metadata": {},
   "source": [
    "### Save on CPU, Load on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90329a3c-614f-4558-be8d-2e5b9abdab6c",
   "metadata": {},
   "source": [
    "Let's say we saved our model on the CPU but later during loading we want t load it to the GPU then we first have to specify the cuda device. Then we create our model and then we call **model.load_state_dict()** with **torch.load(PATH, map_location=\"cuda:0\")** inside. As map location we specify cuda: and any GPU device number we want. After that we also have to call **model.to(device)**. We also have to send all the training samples to the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe76caf1-6f50-413c-8c44-5d22f0c8f5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save on CPU, Load on GPU\n",
    "torch.save(model.state_dict(), PATH)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model = Model(*args, **kwargs)\n",
    "model.load_state_dict(torch.load(PATH, map_location=\"cuda:0\")) # Choose whatever GPU device number you want\n",
    "model.to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
