{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy Word Vectors\n",
    "\n",
    "Based on **Dr. William Mattingly** video: https://www.youtube.com/watch?v=dIUTsFT2MeQ&t\n",
    "\n",
    "and his Jupyter Book: http://spacy.pythonhumanities.com/01_03_word_vectors.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word vectors, also known as word embeddings, are numerical representations of words in a high-dimensional space. They capture semantic and syntactic relationships between words, allowing NLP models to understand the meaning and context of words based on their vector representations. Word vectors are typically learned from large text corpora using techniques like Word2Vec, GloVe, or FastText.\n",
    "\n",
    "1. **Word2Vec**: Word2Vec is a widely used algorithm developed by Tomas Mikolov et al, It offers two architectures: Continuous Bag of Words (CBOW) and Skip-gram. Word2Vec learns word embeddings by predicting the context (neighborin words) given a target word or vice versa. The resulting word vectors are capable of capturing word similarities and analogies.\n",
    "2. **GloVe (Global Vectors for Word Representation)**: GloVe is an algorithm developed by Stanford researchers. It combines global matrix factorization and local context window-based methods to learn word embeddings. GloVe learns word vectors by analyzing global word co-occurrence statistics. It considers the probabilities of word appearing together and constructs a co-occurrence matrix to capture word relationships.\n",
    "3. **FastText**: FastText is an extension of Word2Vec developed by Facebook AI Research. It introduces a subword-level modeling approach by representing words as bag of character n-grams. FastText considers subword information to handle out-of-vocabulary words and can generate embeddings for rare or unseen word based on their character constituents.\n",
    "\n",
    "In the context of spaCy, word vectors play a crucial role in many NLP tasks. spaCy provides pre-trained word vectors for many languages, which can be accessed using **vector** atrribute of a **Token** object. These word vectors enable spaCy models to preform various tasks such as similarity analysis, entity recognition, text classification, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
