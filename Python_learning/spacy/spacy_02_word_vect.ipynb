{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy Word Vectors\n",
    "\n",
    "Based on **Dr. William Mattingly** video: https://www.youtube.com/watch?v=dIUTsFT2MeQ&t\n",
    "\n",
    "and his Jupyter Book: http://spacy.pythonhumanities.com/01_03_word_vectors.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word vectors, also known as word embeddings, are numerical representations of words in a high-dimensional space. They capture semantic and syntactic relationships between words, allowing NLP models to understand the meaning and context of words based on their vector representations. Word vectors are typically learned from large text corpora using techniques like Word2Vec, GloVe, or FastText.\n",
    "\n",
    "1. **Word2Vec**: Word2Vec is a widely used algorithm developed by Tomas Mikolov et al, It offers two architectures: Continuous Bag of Words (CBOW) and Skip-gram. Word2Vec learns word embeddings by predicting the context (neighborin words) given a target word or vice versa. The resulting word vectors are capable of capturing word similarities and analogies.\n",
    "2. **GloVe (Global Vectors for Word Representation)**: GloVe is an algorithm developed by Stanford researchers. It combines global matrix factorization and local context window-based methods to learn word embeddings. GloVe learns word vectors by analyzing global word co-occurrence statistics. It considers the probabilities of word appearing together and constructs a co-occurrence matrix to capture word relationships.\n",
    "3. **FastText**: FastText is an extension of Word2Vec developed by Facebook AI Research. It introduces a subword-level modeling approach by representing words as bag of character n-grams. FastText considers subword information to handle out-of-vocabulary words and can generate embeddings for rare or unseen word based on their character constituents.\n",
    "\n",
    "In the context of spaCy, word vectors play a crucial role in many NLP tasks. spaCy provides pre-trained word vectors for many languages, which can be accessed using **vector** atrribute of a **Token** object. These word vectors enable spaCy models to preform various tasks such as similarity analysis, entity recognition, text classification, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# We need to download a larger model than the one we downloaded last time\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "with open (\"data/wiki_us.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "doc = nlp(text)\n",
    "sentence1 = list(doc.sents)[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors\n",
    "\n",
    "Word vectors, also known as word embeddings, are numerical representations of words in a continuous vector space. Each word in a given language is assigned a fixed-size vector, typically with hundreads of dimensions, where the values in each dimenstion capture different apects of the word's meaning.\n",
    "\n",
    "Word vectors are derived from training models on large amounts of text data, such as books, articles, or web pages. These models learn to assign similar vector representations to words that have similar contexts or meanings. The underlying assumption is that words appearing in similar contexts are likely to have similar meanings.\n",
    "\n",
    "Word vectors have become a fundamental component of many NLP applications. They enable machines to capture semantic relationships between words. such as analogies or similarities, and perform various language-related tasks. By representing words as dense vectors, it becomes possible to perform mathematical operations on them, such as vector addition and subtraction, to explore relationships between words.\n",
    "\n",
    "Word vectors have several advantages in NLP tasks. They can improve the performance of the models in tasks like language modeling, sentiment analysis, machine translation, NER, and document classification. They also help in capturing semantic properties of wrods, allowing models to generalize better and handle out-of-vocabulary words.\n",
    "\n",
    "Populat lagorithms for learning word vectors include Word2Vec, GloVe, and FastText. These algorithms have been successful in capturing word semantics and have pre-trained models available for various languages, making i easier to incorporate word vector into NLP applications."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial methods for creating word vectors in a pipeline take all words in a corpus and convert them into a single, unique number. These words are then stored in a dictionary that would look like this: {\"the\":1, \"a\":2} etc. This is known as a bag of words. This approach to representing words numerically, however, only allow a computer to understand words numerically to identify unique words. It does not, however, allow a computer to understand meaning.\n",
    "\n",
    "sentences:\n",
    "\n",
    "Tom loves to eat chocolate.\n",
    "\n",
    "Tom likes to eat chocolate.\n",
    "\n",
    "List representation:\n",
    "\n",
    "1, 2, 3, 4, 5\n",
    "\n",
    "1, 6, 3, 4, 5\n",
    "\n",
    "Both sentences are nearly identical. The only difference is the degree to which Tom appreciates eating chocolate. If we examine the numbers, however, these two sentences semm quite close, but their semantical meaning is impossible to know for certain. How similar is 2 to 6? The number 6 could represent \"hates\" as much as it could represent \"likes\". This is where word vectors come in.\n",
    "\n",
    "Word vectors transform one-dimensional bag-of-words representation into multi-dimensional representations by mapping them to higher-dimensional spaces using machine learning techniques.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Word Vectors?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of word vectors is to enable computers to comprehend language numerically, enabling them to perform more advanced tasks on textual data. To illustrate this let's consider the scenarion mentioned aboce. One possible solution to help computer understand that **2** and **6** are synonymous or have similar meanings could be to provide the computer synonyms and thereby understand word meanings. While this approach may seem logical, it is not a viable solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tom has no Synonyms in the API\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m words:\n\u001b[1;32m     10\u001b[0m     syns \u001b[39m=\u001b[39m dictionary\u001b[39m.\u001b[39msynonym(word)\n\u001b[0;32m---> 11\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWord: \u001b[39m\u001b[39m{\u001b[39;00mword\u001b[39m:\u001b[39;00m\u001b[39m<10\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, synonyms: \u001b[39m\u001b[39m{\u001b[39;00msyns[\u001b[39m0\u001b[39;49m:\u001b[39m5\u001b[39;49m]\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Looking for synonyms in PyDictionary\n",
    "from PyDictionary import PyDictionary\n",
    "\n",
    "dictionary=PyDictionary()\n",
    "\n",
    "text = \"Tom loves to eat chocolate\"\n",
    "\n",
    "words = text.split()\n",
    "for word in words:\n",
    "    syns = dictionary.synonym(word)\n",
    "    print(f\"Word: {word:<10}, synonyms: {syns[0:5]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like has no Synonyms in the API\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m words:\n\u001b[1;32m      7\u001b[0m     syns \u001b[39m=\u001b[39m dictionary\u001b[39m.\u001b[39msynonym(word)\n\u001b[0;32m----> 8\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWord: \u001b[39m\u001b[39m{\u001b[39;00mword\u001b[39m:\u001b[39;00m\u001b[39m<10\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, synonyms: \u001b[39m\u001b[39m{\u001b[39;00msyns[\u001b[39m0\u001b[39;49m:\u001b[39m5\u001b[39;49m]\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from PyDictionary import PyDictionary\n",
    "\n",
    "dictionary = PyDictionary()\n",
    "\n",
    "words = [\"like\", \"love\"]\n",
    "for word in words:\n",
    "    syns = dictionary.synonym(word)\n",
    "    print(f\"Word: {word:<10}, synonyms: {syns[0:5]}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apperance of Word Vectors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Words vectors have a fixed number of dimensions, which are determined through machine learning techniques.\n",
    "+ Machine learning models consider various factors such as word frequency, co-occurrence of words within a corpus, and contextual similarites to shape the dimension of word vectors.\n",
    "+ Word vectors enable the computer to measure syntactical similarity between words using numerical values.\n",
    "+ To represent these relationships numerically, word vectors are often structured as a matrix of matrices, commonly known as a tensor.\n",
    "+ To make the representation more concise, models flatten the matrix into a single floating-point number, typically a decimal.\n",
    "+ The number of dimensions in word vectors corresponds to the number of floating-point values in the flattened matrix, thus influencing the richness anc complexity of the representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-7.2681e+00, -8.5717e-01,  5.8105e+00,  1.9771e+00,  8.8147e+00,\n",
       "       -5.8579e+00,  3.7143e+00,  3.5850e+00,  4.7987e+00, -4.4251e+00,\n",
       "        1.7461e+00, -3.7296e+00, -5.1407e+00, -1.0792e+00, -2.5555e+00,\n",
       "        3.0755e+00,  5.0141e+00,  5.8525e+00,  7.3378e+00, -2.7689e+00,\n",
       "       -5.1641e+00, -1.9879e+00,  2.9782e+00,  2.1024e+00,  4.4306e+00,\n",
       "        8.4355e-01, -6.8742e+00, -4.2949e+00, -1.7294e-01,  3.6074e+00,\n",
       "        8.4379e-01,  3.3419e-01, -4.8147e+00,  3.5683e-02, -1.3721e+01,\n",
       "       -4.6528e+00, -1.4021e+00,  4.8342e-01,  1.2549e+00, -4.0644e+00,\n",
       "        3.3278e+00, -2.1590e-01, -5.1786e+00,  3.5360e+00, -3.1575e+00,\n",
       "       -3.5273e+00, -3.6753e+00,  1.5863e+00, -8.1594e+00, -3.4657e+00,\n",
       "        1.5262e+00,  4.8135e+00, -3.8428e+00, -3.9082e+00,  6.7549e-01,\n",
       "       -3.5787e-01, -1.7806e+00,  3.5284e+00, -5.1114e-02, -9.7150e-01,\n",
       "       -9.0553e-01, -1.5570e+00,  1.2038e+00,  4.7708e+00,  9.8561e-01,\n",
       "       -2.3186e+00, -7.4899e+00, -9.5389e+00,  8.5572e+00,  2.7420e+00,\n",
       "       -3.6270e+00,  2.7456e+00, -6.9574e+00, -1.7190e+00, -2.9145e+00,\n",
       "        1.1838e+00,  3.7864e+00,  2.0413e+00, -3.5808e+00,  1.4319e+00,\n",
       "        2.0528e-01, -7.0640e-01, -5.3556e+00, -2.5911e+00,  4.4922e+00,\n",
       "        1.6574e+00,  3.9794e+00, -4.3560e+00, -2.7266e+00,  1.9581e+00,\n",
       "       -3.4842e+00, -3.9674e+00,  3.2690e+00,  6.6683e-01,  3.9837e+00,\n",
       "       -6.5997e+00,  4.1630e+00,  8.0338e+00,  3.8102e-01,  8.2656e+00,\n",
       "        9.7061e-01, -5.0807e+00,  4.9522e+00,  7.5018e+00,  3.8305e+00,\n",
       "       -3.3233e+00,  4.9126e+00,  2.4189e-01,  3.8218e+00, -3.9717e+00,\n",
       "        2.4691e+00,  1.3721e+01, -8.9664e+00,  1.0610e+01,  6.9425e-01,\n",
       "       -1.1082e+01, -5.6883e+00,  2.3287e+00,  1.6451e+00,  3.6006e+00,\n",
       "        1.2588e-01, -6.1956e+00,  1.1455e+01,  5.6682e+00, -5.0251e-01,\n",
       "       -9.8515e-01,  8.8902e-02, -4.0213e+00,  3.6134e+00, -9.0936e+00,\n",
       "       -1.4555e+01, -2.5591e+00,  4.0959e+00, -3.5929e-01,  1.0219e+00,\n",
       "        3.9402e+00,  8.0495e-01, -3.6023e+00,  2.6394e+00, -1.5258e-01,\n",
       "       -2.6182e+00, -2.6268e-01, -2.1610e+00,  2.3950e+00,  6.8842e+00,\n",
       "        3.6034e+00,  1.8058e+00,  2.4528e+00,  4.4088e+00, -1.0598e+00,\n",
       "        6.4964e+00,  5.9196e+00, -1.0261e+00, -1.7013e+00, -4.4151e+00,\n",
       "        4.3043e+00, -1.7138e+00, -4.6690e+00, -5.5212e-01,  5.3995e+00,\n",
       "        1.8311e+00, -3.5820e-01, -3.6578e-01, -2.8578e+00, -6.4639e+00,\n",
       "       -3.2155e+00,  6.7083e-01, -1.2800e+00,  1.2782e+00,  7.8274e-01,\n",
       "        1.9839e-01, -1.4163e+00,  2.1184e+00,  1.5021e+00, -1.8212e+00,\n",
       "        1.6629e+00,  4.0354e+00, -4.4648e+00, -3.4897e+00, -2.5765e+00,\n",
       "       -3.6317e+00, -4.1619e-02,  4.8660e-01,  2.0712e+00, -1.9166e+00,\n",
       "       -3.4045e+00, -7.6609e+00, -2.1940e+00, -2.3919e-03,  8.4900e-01,\n",
       "        1.3921e+00, -5.7830e+00,  4.4739e+00,  1.0642e+00,  5.7864e+00,\n",
       "        3.4643e+00, -5.9169e+00, -2.6925e+00, -1.1271e-01, -6.0462e+00,\n",
       "        3.9285e+00, -3.0423e+00, -6.9939e-02,  2.2826e-01,  8.0214e+00,\n",
       "        2.2098e+00, -1.1049e+01,  7.6001e-02, -1.5970e+00,  2.0524e-01,\n",
       "        2.8063e+00,  3.5245e+00, -3.9300e+00, -9.7995e-01,  4.0248e+00,\n",
       "        1.8447e+00, -2.0452e+00,  1.1419e+00, -4.4600e-01, -9.5551e-01,\n",
       "       -1.0224e+00,  5.9224e+00, -6.1688e+00, -8.3840e-01, -7.9102e+00,\n",
       "       -8.9575e-02, -2.7741e-01,  4.2703e+00,  4.0212e+00, -1.1166e-01,\n",
       "        2.5119e+00, -5.9635e+00, -1.2320e+00,  2.8199e-01, -4.1062e+00,\n",
       "       -6.2923e-01, -5.2420e-01,  2.5213e+00, -3.5094e+00,  6.4333e+00,\n",
       "        7.9466e+00, -3.3883e+00,  5.2535e+00,  9.4524e-02, -3.3336e+00,\n",
       "        5.9621e+00, -1.0794e+00, -6.0850e+00, -3.6071e+00, -3.8496e-01,\n",
       "        7.6137e+00, -9.1081e+00, -6.0037e+00, -2.4735e+00, -6.5050e-01,\n",
       "       -6.3021e+00,  8.5783e+00,  1.7250e-01,  4.3631e+00, -9.3439e+00,\n",
       "        2.0984e-01,  7.6900e-01,  1.0763e+01,  4.4598e-01, -3.6584e+00,\n",
       "       -3.0992e+00, -3.8868e+00,  4.3337e+00, -5.8037e+00, -1.1337e+00,\n",
       "       -6.1562e+00,  3.1820e-01, -1.0612e+00, -1.4809e+00,  6.0373e+00,\n",
       "        4.6015e-01, -1.5530e+00, -1.0562e+00,  5.8618e-01,  3.4431e+00,\n",
       "        4.5542e+00, -3.1881e+00, -1.5832e+00,  3.0859e+00,  1.3061e+00,\n",
       "       -8.0091e+00,  7.7996e+00, -5.0644e+00,  8.8719e+00,  7.2337e-01,\n",
       "       -1.2350e+00,  1.6209e+00,  7.8994e+00,  1.0741e+01,  8.1158e-01,\n",
       "        9.0156e+00, -1.5913e+00, -5.3166e+00,  3.5032e-01, -2.8850e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first word\n",
    "sentence1[0].vector"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a word vector model is trained, we can do similarity matches very quickly and very reliably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dogsbody', 'wolfdogs', 'Baeg', 'duppy', 'pet(s', 'postcanine', 'Kebira', 'uppies', 'Toropets', 'moggie']\n"
     ]
    }
   ],
   "source": [
    "# Words most closely related to the word dog\n",
    "import numpy as np\n",
    "\n",
    "my_word1 = \"dog\"\n",
    "\n",
    "ms = nlp.vocab.vectors.most_similar(\n",
    "    np.asarray([nlp.vocab.vectors[nlp.vocab.strings[my_word1]]]), n=10)\n",
    "words = [nlp.vocab.strings[w] for w in ms[0][0]]\n",
    "distances = ms[2]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['country—0,467', 'nationâ\\x80\\x99s', 'countries-', 'continente', 'Carnations', 'pastille', 'бесплатно', 'Argents', 'Tywysogion', 'Teeters']\n"
     ]
    }
   ],
   "source": [
    "my_word2 = \"country\"\n",
    "\n",
    "ms = nlp.vocab.vectors.most_similar(\n",
    "    np.asarray([nlp.vocab.vectors[nlp.vocab.strings[my_word2]]]), n=10)\n",
    "words = [nlp.vocab.strings[w] for w in ms[0][0]]\n",
    "distances = ms[2]\n",
    "print(words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc Similarity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In spaCy we can do this same thin at the document level. Through word vectors we can calculate the similarity between two documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like salty fries and hamburgers. <-> Fast food tastes very good. 0.691649353055761\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "doc1 = nlp(\"I like salty fries and hamburgers.\")\n",
    "doc2 = nlp(\"Fast food tastes very good.\")\n",
    "\n",
    "# Similarity of two documents\n",
    "print(doc1, \"<->\", doc2, doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like salty fries and hamburgers. <-> The Empire State Building is in New York. 0.1766669125394067\n"
     ]
    }
   ],
   "source": [
    "doc3 = nlp(\"The Empire State Building is in New York.\")\n",
    "\n",
    "print(doc1, \"<->\", doc3, doc1.similarity(doc3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I enjoy oranges. <-> I enjoy apples. 0.977570143948367\n"
     ]
    }
   ],
   "source": [
    "doc4 = nlp(\"I enjoy oranges.\")\n",
    "doc5 = nlp(\"I enjoy apples.\")\\\n",
    "\n",
    "print(doc4, \"<->\", doc5, doc4.similarity(doc5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I enjoy oranges. <-> I enjoy burgers. 0.9628306772893752\n"
     ]
    }
   ],
   "source": [
    "doc6 = nlp(\"I enjoy burgers.\")\n",
    "\n",
    "\n",
    "print(doc4, \"<->\", doc6, doc4.similarity(doc6))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apples and orages are in a similar cluster category around the fruit because of their word embedding."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Similarity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also calculate the similarity betwen two given words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salty fries <-> hamburgers 0.6938489079475403\n"
     ]
    }
   ],
   "source": [
    "# Similarity of tokens and spans\n",
    "french_fries = doc1[2:4]\n",
    "burgers = doc1[5]\n",
    "print(french_fries, \"<->\", burgers, french_fries.similarity(burgers))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
