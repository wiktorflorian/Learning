{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy's Matcher\n",
    "\n",
    "Based on **Dr. William Mattingly** video: https://www.youtube.com/watch?v=dIUTsFT2MeQ&t\n",
    "\n",
    "and his Jupyter Book: http://spacy.pythonhumanities.com/02_02_matcher.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexeme"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **lexeme** in spaCy represents a word in a text and includes essential linguistic attributes. It serves as a unit of vocabulary and is associated with a unique integer ID. Lexemes store information such as the word's text, part-of-speech tag, lemma, morphological features and more. They enable efficient and memory-friendly text processing by serving as shared references to the vocabulary, minimizing duplication of linguistic information. Working with lexemes in spaCy enhances performance and reduces memory usage in natural language processing tasks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"LIKE_EMAIL\": True}]\n",
    "matcher.add(\"EMAIL_ADDRESS\", [pattern])\n",
    "doc = nlp(\"This is an email address: wiktorflorianwf@gmail.com\")\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexeme: 16571425990740197027, start token: 6, end token: 7\n"
     ]
    }
   ],
   "source": [
    "print(f\"Lexeme: {matches[0][0]}, start token: {matches[0][1]}, end token: {matches[0][2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexeme: EMAIL_ADDRESS, start token: IS_SPACE, end token: IS_TITLE\n"
     ]
    }
   ],
   "source": [
    "print(f\"Lexeme: {nlp.vocab[matches[0][0]].text}, start token: {nlp.vocab[matches[0][1]].text}, end token: {nlp.vocab[matches[0][2]].text}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atrributes of the Matcher"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **ORTH**: The exact verbatim of a token (string). the token's \"orthographic\" form, which is the exact verbatim representation of the token as it appears in the original text. It preserves the original casing, punctuation, and any other textual details without modifications. The **ORTH** attribute is useful when you want to precisely match or manipulate the token's original form. \n",
    "+ **TEXT**: The exat verbatim of a token (string). Normalized form of token, typically returns the lowercase version of the token, regardless of its original casing. The **TEXT** atrribute is useful when you want to compare or process tokens in a case-insensitive manner or when you want to apply general text processing operations.\n",
    "+ **LOWER**: The lowercase form of the token text (string).\n",
    "+ **LENGTH**: The length of the token text (integer).\n",
    "+ **IS_ALPHA**: Indicates if the token consists of alphabetic characters.\n",
    "+ **IS_ASCII**: Indicates if the token consists of ASCII characters.\n",
    "+ **IS_DIGIT**: Indicates if the token consists of digits.\n",
    "+ **IS_LOWER**: Indicates if the token is in lowercase.\n",
    "+ **IS_UPPER**: Indicates if the token is in uppercase.\n",
    "+ **IS_TITLE**: Indicates if the token is in title case.\n",
    "+ **IS_PUNCT**: Indicates if the token is a punctuaction mark.\n",
    "+ **IS_SPACE**: Indicates if the token is a space character.\n",
    "+ **IS_STOP**: Indicates if the token is a stop word.\n",
    "+ **IS_SENT_START**: Indicates if the token starts a sentence.\n",
    "+ **LIKE_NUM**: Indicates if the token resembles a numeric value.\n",
    "+ **LIKE_URL**: Indicates if the token resembles a URL.\n",
    "+ **LIKE_EMAIL**: Indicates if the token resembles an email address.\n",
    "+ **SPACY**: The unique identifier of the spaCy model.\n",
    "+ **POS**: The part-of-speech tag of token.\n",
    "+ **TAG**: The fine-grained part-of-speech tag of token.\n",
    "+ **MORPH**: The morphological features of the token.\n",
    "+ **DEP**: The syntactic dependency relation of the token.\n",
    "+ **LEMMA**: The base form or lemma of the token\n",
    "+ **SHAPE**: The shape or pattern of the token.\n",
    "+ **ENT_TYPE**: The named entity type of the token.\n",
    "+ **_**: Custom extension attributes (a dictionaryy of strin keys and any values).\n",
    "+ **OP**: The operator used to define the matching pattern."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applied Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"data/wiki_mlk.txt\", \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grabbing all Proper Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102\n",
      "Lexeme: 3232560085755078826, start token: 0, end token: 1, proper noun: Martin\n",
      "Lexeme: 3232560085755078826, start token: 1, end token: 2, proper noun: Luther\n",
      "Lexeme: 3232560085755078826, start token: 2, end token: 3, proper noun: King\n",
      "Lexeme: 3232560085755078826, start token: 3, end token: 4, proper noun: Jr.\n",
      "Lexeme: 3232560085755078826, start token: 6, end token: 7, proper noun: Michael\n",
      "Lexeme: 3232560085755078826, start token: 7, end token: 8, proper noun: King\n",
      "Lexeme: 3232560085755078826, start token: 8, end token: 9, proper noun: Jr.\n",
      "Lexeme: 3232560085755078826, start token: 10, end token: 11, proper noun: January\n",
      "Lexeme: 3232560085755078826, start token: 15, end token: 16, proper noun: April\n",
      "Lexeme: 3232560085755078826, start token: 23, end token: 24, proper noun: Baptist\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"POS\": \"PROPN\"}]\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern])\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "print(len(matches))\n",
    "for match in matches[:10]:\n",
    "    print(f\"Lexeme: {match[0]}, start token: {match[1]}, end token: {match[2]}, proper noun: {doc[match[1]:match[2]]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Word Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175\n",
      "Lexeme: 3232560085755078826, start token: 0, end token: 1, proper noun: Martin\n",
      "Lexeme: 3232560085755078826, start token: 0, end token: 2, proper noun: Martin Luther\n",
      "Lexeme: 3232560085755078826, start token: 1, end token: 2, proper noun: Luther\n",
      "Lexeme: 3232560085755078826, start token: 0, end token: 3, proper noun: Martin Luther King\n",
      "Lexeme: 3232560085755078826, start token: 1, end token: 3, proper noun: Luther King\n",
      "Lexeme: 3232560085755078826, start token: 2, end token: 3, proper noun: King\n",
      "Lexeme: 3232560085755078826, start token: 0, end token: 4, proper noun: Martin Luther King Jr.\n",
      "Lexeme: 3232560085755078826, start token: 1, end token: 4, proper noun: Luther King Jr.\n",
      "Lexeme: 3232560085755078826, start token: 2, end token: 4, proper noun: King Jr.\n",
      "Lexeme: 3232560085755078826, start token: 3, end token: 4, proper noun: Jr.\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"POS\": \"PROPN\", \"OP\": \"+\"}]\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern])\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "print(len(matches))\n",
    "for match in matches[:10]:\n",
    "    print(f\"Lexeme: {match[0]}, start token: {match[1]}, end token: {match[2]}, proper noun: {doc[match[1]:match[2]]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy Keyword Argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n",
      "Lexeme: 3232560085755078826, start token: 83 , end token: 88 , proper noun: Martin Luther King Sr.\n",
      "Lexeme: 3232560085755078826, start token: 469, end token: 474, proper noun: Martin Luther King Jr. Day\n",
      "Lexeme: 3232560085755078826, start token: 536, end token: 541, proper noun: Martin Luther King Jr. Memorial\n",
      "Lexeme: 3232560085755078826, start token: 0  , end token: 4  , proper noun: Martin Luther King Jr.\n",
      "Lexeme: 3232560085755078826, start token: 128, end token: 132, proper noun: Southern Christian Leadership Conference\n",
      "Lexeme: 3232560085755078826, start token: 247, end token: 251, proper noun: Director J. Edgar Hoover\n",
      "Lexeme: 3232560085755078826, start token: 6  , end token: 9  , proper noun: Michael King Jr.\n",
      "Lexeme: 3232560085755078826, start token: 325, end token: 328, proper noun: Nobel Peace Prize\n",
      "Lexeme: 3232560085755078826, start token: 422, end token: 425, proper noun: James Earl Ray\n",
      "Lexeme: 3232560085755078826, start token: 463, end token: 466, proper noun: Congressional Gold Medal\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"POS\": \"PROPN\", \"OP\": \"+\"}]\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern], greedy=\"LONGEST\")\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "print(len(matches))\n",
    "for match in matches[:10]:\n",
    "    print(f\"Lexeme: {match[0]:<19}, start token: {match[1]:<3}, end token: {match[2]:<3}, proper noun: {doc[match[1]:match[2]]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n",
      "Lexeme: 3232560085755078826, start token: 0  , end token: 4  , proper noun: Martin Luther King Jr.\n",
      "Lexeme: 3232560085755078826, start token: 6  , end token: 9  , proper noun: Michael King Jr.\n",
      "Lexeme: 3232560085755078826, start token: 10 , end token: 11 , proper noun: January\n",
      "Lexeme: 3232560085755078826, start token: 15 , end token: 16 , proper noun: April\n",
      "Lexeme: 3232560085755078826, start token: 23 , end token: 24 , proper noun: Baptist\n",
      "Lexeme: 3232560085755078826, start token: 49 , end token: 50 , proper noun: King\n",
      "Lexeme: 3232560085755078826, start token: 69 , end token: 71 , proper noun: Mahatma Gandhi\n",
      "Lexeme: 3232560085755078826, start token: 83 , end token: 88 , proper noun: Martin Luther King Sr.\n",
      "Lexeme: 3232560085755078826, start token: 89 , end token: 90 , proper noun: King\n",
      "Lexeme: 3232560085755078826, start token: 113, end token: 114, proper noun: King\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"POS\": \"PROPN\", \"OP\": \"+\"}]\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern], greedy=\"LONGEST\")\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "matches.sort(key = lambda x: x[1])\n",
    "print(len(matches))\n",
    "for match in matches[:10]:\n",
    "    print(f\"Lexeme: {match[0]:<19}, start token: {match[1]:<3}, end token: {match[2]:<3}, proper noun: {doc[match[1]:match[2]]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding in Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "Lexeme: 3232560085755078826, start token: 49 , end token: 51 , proper noun: King advanced\n",
      "Lexeme: 3232560085755078826, start token: 89 , end token: 91 , proper noun: King participated\n",
      "Lexeme: 3232560085755078826, start token: 113, end token: 115, proper noun: King led\n",
      "Lexeme: 3232560085755078826, start token: 167, end token: 169, proper noun: King helped\n",
      "Lexeme: 3232560085755078826, start token: 247, end token: 252, proper noun: Director J. Edgar Hoover considered\n",
      "Lexeme: 3232560085755078826, start token: 322, end token: 324, proper noun: King won\n",
      "Lexeme: 3232560085755078826, start token: 485, end token: 488, proper noun: United States beginning\n"
     ]
    }
   ],
   "source": [
    "matcher  = Matcher(nlp.vocab)\n",
    "pattern = [{\"POS\": \"PROPN\", \"OP\": \"+\"}, {\"POS\": \"VERB\"}]\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern], greedy=\"LONGEST\")\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "matches.sort(key = lambda x: x[1])\n",
    "print(len(matches))\n",
    "for match in matches[:10]:\n",
    "    print(f\"Lexeme: {match[0]:<19}, start token: {match[1]:<3}, end token: {match[2]:<3}, proper noun: {doc[match[1]:match[2]]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quotes and Speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"data/alice.json\", \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, `and what is the use of a book,' thought Alice `without pictures or conversation?'\n"
     ]
    }
   ],
   "source": [
    "text = data[0][2][0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, 'and what is the use of a book,' thought Alice 'without pictures or conversation?'\n"
     ]
    }
   ],
   "source": [
    "text = data[0][2][0].replace(\"`\", \"'\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Lexeme: 3232560085755078826, start token: 47 , end token: 58 , proper noun: 'and what is the use of a book,'\n",
      "Lexeme: 3232560085755078826, start token: 60 , end token: 67 , proper noun: 'without pictures or conversation?'\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"ORTH\": \"'\"}, {\"IS_ALPHA\": True, \"OP\": \"+\"}, {\"IS_PUNCT\": True, \"OP\": \"*\"}, {\"ORTH\": \"'\"}]\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern], greedy=\"LONGEST\")\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "matches.sort(key = lambda x: x[1])\n",
    "print(len(matches))\n",
    "for match in matches:\n",
    "    print(f\"Lexeme: {match[0]:<19}, start token: {match[1]:<3}, end token: {match[2]:<3}, proper noun: {doc[match[1]:match[2]]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Lexeme: 3232560085755078826, start token: 47 , end token: 67 , proper noun: 'and what is the use of a book,' thought Alice 'without pictures or conversation?'\n"
     ]
    }
   ],
   "source": [
    "speak_lemmas = [\"think\", \"say\"]\n",
    "text = data[0][2][0].replace(\"`\", \"'\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern1 = [{\"ORTH\": \"'\"}, {\"IS_ALPHA\": True, \"OP\": \"+\"}, {\"IS_PUNCT\": True, \"OP\": \"*\"}, {\"ORTH\": \"'\"},\n",
    "            {\"POS\": \"VERB\", \"LEMMA\": {\"IN\": speak_lemmas}}, {\"POS\": \"PROPN\", \"OP\": \"+\"}, {\"ORTH\": \"'\"},\n",
    "            {\"IS_ALPHA\": True, \"OP\": \"+\"}, {\"IS_PUNCT\": True, \"OP\": \"*\"}, {\"ORTH\": \"'\"}]\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern1], greedy=\"LONGEST\")\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "matches.sort(key = lambda x: x[1])\n",
    "print(len(matches))\n",
    "for match in matches:\n",
    "    print(f\"Lexeme: {match[0]:<19}, start token: {match[1]:<3}, end token: {match[2]:<3}, proper noun: {doc[match[1]:match[2]]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem with this Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(3232560085755078826, 47, 67) 'and what is the use of a book,' thought Alice 'without pictures or conversation?'\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for text in data[0][2]:\n",
    "    text = text.replace(\"`\", \"'\")\n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "    matches.sort(key = lambda x: x[1])\n",
    "    print(len(matches))\n",
    "    for match in matches[:10]:\n",
    "        print(match, doc[match[1]:match[2]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding More Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(3232560085755078826, 47, 67) 'and what is the use of a book,' thought Alice 'without pictures or conversation?'\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "(3232560085755078826, 0, 6) 'Well!' thought Alice\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "(3232560085755078826, 57, 68) 'which certainly was not here before,' said Alice\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "speak_lemmas = [\"think\", \"say\"]\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern1 = [{\"ORTH\": \"'\"}, {\"IS_ALPHA\": True, \"OP\": \"+\"}, {\"IS_PUNCT\": True, \"OP\": \"*\"}, {\"ORTH\": \"'\"},\n",
    "            {\"POS\": \"VERB\", \"LEMMA\": {\"IN\": speak_lemmas}}, {\"POS\": \"PROPN\", \"OP\": \"+\"}, {\"ORTH\": \"'\"},\n",
    "            {\"IS_ALPHA\": True, \"OP\": \"+\"}, {\"IS_PUNCT\": True, \"OP\": \"*\"}, {\"ORTH\": \"'\"}]\n",
    "pattern2 = [{\"ORTH\": \"'\"}, {\"IS_ALPHA\": True, \"OP\": \"+\"}, {\"IS_PUNCT\": True, \"OP\": \"*\"}, {\"ORTH\": \"'\"},\n",
    "            {\"POS\": \"VERB\", \"LEMMA\": {\"IN\": speak_lemmas}}, {\"POS\": \"PROPN\", \"OP\": \"+\"}]\n",
    "pattern3 = [{\"POS\": \"PROPN\", \"OP\": \"+\"}, {\"POS\": \"VERB\", \"LEMMA\": {\"IN\": speak_lemmas}}, {\"ORTH\": \"'\"},\n",
    "            {\"IS_ALPHA\": True, \"OP\": \"+\"}, {\"IS_PUNCT\": True, \"OP\": \"*\"}, {\"ORTH\": \"'\"}]\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern1, pattern2, pattern3], greedy=\"LONGEST\")\n",
    "for text in data[0][2]:\n",
    "    text = text.replace(\"`\", \"'\")\n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "    matches.sort(key = lambda x: x[1])\n",
    "    print(len(matches))\n",
    "    for match in matches[:10]:\n",
    "        print(match, doc[match[1]:match[2]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
