{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy's Pipelines\n",
    "\n",
    "Based on **Dr. William Mattingly** video: https://www.youtube.com/watch?v=dIUTsFT2MeQ&t\n",
    "\n",
    "and his Jupyter Book: http://spacy.pythonhumanities.com/01_04_pipelines.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Pipes (Components and Factories) Available from spaCy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ spaCy serves as both an NLP framework and a system for constructing and executing intricate pipelines.\n",
    "+ A pipeline in spaCy is a series of pipes or actors that perform operations on data, either modifying it or extracting relevant information.\n",
    "+ Some pipes in the pipeline may rely on the output generated by preceding pipes, hile others can operata independently.\n",
    "\n",
    "The image provides an example of spaCy pipiline, showcasing the components and their flow of data\n",
    "\n",
    "<div style=\"display: flex; justify-content: center\">\n",
    "<img src=\"images/spacy_03_01.png\" alt=\"spaCy pipeline\" width=\"700\" height=\"275\">\n",
    "</div>\n",
    "\n",
    "+ In the described pipeline, an input sentence enters from the left.\n",
    "+ The pipeline consists of two activated pipes:\n",
    "    + An EntityRuler, whichj is a rule-based NER that identifies entities in the sentence.\n",
    "    + An EntityLinker pipe, which performs topoym resolution to determine the specific entity.\n",
    "+ After passing through these pipes, the sentence is outputted with annotations indicating the identified entities.\n",
    "+ The spaCy feature **doc.ents** can be used to extract the entities from the annotated sentence.\n",
    "+ In more sophisticated pipelines, spaCy utilizes a Tok2Vec input layer to vectorize the input sentence. This enables machine learning pipes to make predictions based on the vectorized representations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attribute Rullers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attribute Rullers** in spacy are components that allow you to add or modify token attributes based on specific rules. They providea a way to programmatically define and apply rules to tokens in a text. Attribute Rulers can be used to set, overwrite, or extend token attributes such as POS tags, entity labels, dependency labels, and more. They are useful for customizing and fine-tuning token attributes to suit specific NLP tasks or domain-specific requirements."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **Dependency Parser**: Analyzes the grammatical structure and relationships between words in a sentence.\n",
    "+ **EntityLinker**: Links recognized entities to knowledge bases or ontologies for further analysis.\n",
    "+ **EntityRecognizer**: Identifies and classifies named entities in text, such as person names, organizations, or locations.\n",
    "+ **EntityRuler**: Matches and annotates pre-defined entities or patterns in text.\n",
    "+ **Lemmatizer**: Converts words to their base or dictionary form (lemmas).\n",
    "+ **Morpholog**: Provides morphological analysis of words, including their POS and grammatic features.\n",
    "+ **SentenceRecognizer**: Recognizes sentence boundaries in text.\n",
    "+ **Sentencizer**: Splits text into sentences.\n",
    "+ **SpanCategorizer**: Assigns categories or ;abels to spans of text.\n",
    "+ **Tagger**: Assigns POS tags to words.\n",
    "+ **TextCategorizer**: Classifies text into predefined categories or labels.\n",
    "+ **Tok2Vec**: Converts text into numeric vectors using token-level representations.\n",
    "+ **Tokenizer**: Splits text into individual tokens (words, punctuation, etc.).\n",
    "+ **TrainablePipe**: A trainable processing component that can be customized and fine-tuned for specific tasks.\n",
    "+ **Transformer**: Applies a transformed-based model for text processing, such as language modeling or text classification."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matchers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Matchers** in spaCy are components that allow you to define rules to match and extract linguistic patterns or structures from text. They are designed to identify and extract specific combinations of tokens, based on criteria such as token attributes, POS tags, dependencies, and more. Matchers can be used to extract entites, noun phrases, verb phrases, or any other specific patterns of interest from the text. They provide a powerful tool for pattern-based information extraction and can be combined with other spaCy components to build complex NLP pipelines. Matchers in spaCy include PhraseMatcher, DependencyMatcher, and TokenMatcher."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **DependencyMatcher**: DependencyMatcher is a matcher in spaCy that allows you to define rules based on dependency relationships between tokens in a sentence. it provides a way to match and extract subtrees or patterns based on the syntactic dependencies of tokens. DependencyMatcher can be useful for extracting specific syntactic structures or capturing relationships between tokens in a sentence.\n",
    "+ **Matcher**: Matcher is a versatile matcher in spaCy that allows you to define rules based on token attributes, POS tags, dependencies, and more. It provides a way to match and extract specific combinations of tokens based on the defined criteria. Matcher can be used for various tasks such as entity recognition, phrase extraction, or any other pattern-based extraction from the text.\n",
    "+ **PhraseMatcher**: PharseMatcher is a matcher in spaCy that allows you to define rules based on sequences of tokens or phrases. It provides a way to match and extract specific phrases or sequences of interest from the text. PhraseMatcher is useful for tasks such as NER, extracting predefined phrases, or capturing specific linguistic patterns in the text."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Pipes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ In most scenarios, utilizing an existing pre-trained spaCy model is sufficient.\n",
    "+ However, there are cases where an off-the-shelf model may not meet specific requirements or may perform certain tasks slowly.\n",
    "+ An example is sentence tokenization, particularly when dealing with a lengthy document containing a large number of sentences.\n",
    "+ Even with the small English model, processing a large number of sentences can be time-consuming.\n",
    "+ To address this, a blank English model can be created, and only the Sentencizer component needs to be added to it.\n",
    "+ By doing so, unnecessary computational resources and time are saved since only the Sentencizer compoent will be activated and perform the sentence tokenization task.\n",
    "+ This approach can significantly reduce the processing time from potentially hours using the small model to just minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create blank model with language we have to pass the two letter combination\n",
    "nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x10f936dc0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding pipe\n",
    "nlp.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "s = requests.get(\"https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt\")\n",
    "soup = BeautifulSoup(s.content).text.replace(\"-\\n\", \"\").replace(\"\\n\", \" \")\n",
    "nlp.max_length = 5278439"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94134\n",
      "CPU times: user 5.78 s, sys: 64.8 ms, total: 5.85 s\n",
      "Wall time: 5.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc1 = nlp(soup)\n",
    "print(len(list(doc1.sents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp2 = spacy.load(\"en_core_web_sm\")\n",
    "nlp2.max_length = 5278439"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85957\n",
      "CPU times: user 33.8 s, sys: 14 s, total: 47.9 s\n",
      "Wall time: 56.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc2 = nlp2(soup)\n",
    "print(len(list(doc2.sents)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference in time is almost ten times longer.\n",
    "\n",
    "Often times when we need to find sentences quickly, not necessarily accurately."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining a Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining a pipeline in NLP involves inspecting the components and stages of a sequence of processing steps applied to text data. It includes understanding the data flow, order of processing steps, and output generated at each stage. By examining the pipeline, one can gain insights into how the data is processed, identify techniques and algorithms used, and make informes decisions for better performance. Popular NLP libraries like spaCy and NLTK provide modules for inspecting and understanding the pipeline's components and their attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': {'sentencizer': {'assigns': ['token.is_sent_start', 'doc.sents'],\n",
       "   'requires': [],\n",
       "   'scores': ['sents_f', 'sents_p', 'sents_r'],\n",
       "   'retokenizes': False}},\n",
       " 'problems': {'sentencizer': []},\n",
       " 'attrs': {'token.is_sent_start': {'assigns': ['sentencizer'], 'requires': []},\n",
       "  'doc.sents': {'assigns': ['sentencizer'], 'requires': []}}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.analyze_pipes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': {'tok2vec': {'assigns': ['doc.tensor'],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'tagger': {'assigns': ['token.tag'],\n",
       "   'requires': [],\n",
       "   'scores': ['tag_acc'],\n",
       "   'retokenizes': False},\n",
       "  'parser': {'assigns': ['token.dep',\n",
       "    'token.head',\n",
       "    'token.is_sent_start',\n",
       "    'doc.sents'],\n",
       "   'requires': [],\n",
       "   'scores': ['dep_uas',\n",
       "    'dep_las',\n",
       "    'dep_las_per_type',\n",
       "    'sents_p',\n",
       "    'sents_r',\n",
       "    'sents_f'],\n",
       "   'retokenizes': False},\n",
       "  'attribute_ruler': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'lemmatizer': {'assigns': ['token.lemma'],\n",
       "   'requires': [],\n",
       "   'scores': ['lemma_acc'],\n",
       "   'retokenizes': False},\n",
       "  'ner': {'assigns': ['doc.ents', 'token.ent_iob', 'token.ent_type'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False}},\n",
       " 'problems': {'tok2vec': [],\n",
       "  'tagger': [],\n",
       "  'parser': [],\n",
       "  'attribute_ruler': [],\n",
       "  'lemmatizer': [],\n",
       "  'ner': []},\n",
       " 'attrs': {'doc.ents': {'assigns': ['ner'], 'requires': []},\n",
       "  'token.ent_iob': {'assigns': ['ner'], 'requires': []},\n",
       "  'token.ent_type': {'assigns': ['ner'], 'requires': []},\n",
       "  'token.head': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.tag': {'assigns': ['tagger'], 'requires': []},\n",
       "  'token.lemma': {'assigns': ['lemmatizer'], 'requires': []},\n",
       "  'doc.sents': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.dep': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.is_sent_start': {'assigns': ['parser'], 'requires': []},\n",
       "  'doc.tensor': {'assigns': ['tok2vec'], 'requires': []}}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp2.analyze_pipes()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The dictionary structure provides informaton about the pipeline components and their order.\n",
    "+ Each key after \"summary\" represent a pipe in the pipeline, and the corresponding value is a dictionary providing additional details.\n",
    "+ The value dictionary for each pipe contains the following information:\n",
    "    + **assigns**: Specifies the value that the particular pipe assigns to the token and doc as they pass through the pipeline.\n",
    "    + In some cases, there may be a key named **scores** in the dictionary.\n",
    "        + This indicates how the machine learning model was evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp3 = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': {'tok2vec': {'assigns': ['doc.tensor'],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'tagger': {'assigns': ['token.tag'],\n",
       "   'requires': [],\n",
       "   'scores': ['tag_acc'],\n",
       "   'retokenizes': False},\n",
       "  'parser': {'assigns': ['token.dep',\n",
       "    'token.head',\n",
       "    'token.is_sent_start',\n",
       "    'doc.sents'],\n",
       "   'requires': [],\n",
       "   'scores': ['dep_uas',\n",
       "    'dep_las',\n",
       "    'dep_las_per_type',\n",
       "    'sents_p',\n",
       "    'sents_r',\n",
       "    'sents_f'],\n",
       "   'retokenizes': False},\n",
       "  'attribute_ruler': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'lemmatizer': {'assigns': ['token.lemma'],\n",
       "   'requires': [],\n",
       "   'scores': ['lemma_acc'],\n",
       "   'retokenizes': False},\n",
       "  'ner': {'assigns': ['doc.ents', 'token.ent_iob', 'token.ent_type'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False}},\n",
       " 'problems': {'tok2vec': [],\n",
       "  'tagger': [],\n",
       "  'parser': [],\n",
       "  'attribute_ruler': [],\n",
       "  'lemmatizer': [],\n",
       "  'ner': []},\n",
       " 'attrs': {'doc.ents': {'assigns': ['ner'], 'requires': []},\n",
       "  'token.ent_iob': {'assigns': ['ner'], 'requires': []},\n",
       "  'token.ent_type': {'assigns': ['ner'], 'requires': []},\n",
       "  'token.head': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.tag': {'assigns': ['tagger'], 'requires': []},\n",
       "  'token.lemma': {'assigns': ['lemmatizer'], 'requires': []},\n",
       "  'doc.sents': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.dep': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.is_sent_start': {'assigns': ['parser'], 'requires': []},\n",
       "  'doc.tensor': {'assigns': ['tok2vec'], 'requires': []}}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp3.analyze_pipes()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
