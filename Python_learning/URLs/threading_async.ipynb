{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threading and Asynchronous Programming"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note book designed for exploring Threading and Asynchronous Programming."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thread"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **thread** is a separate flow of execution. This means that our program will have two things happening at once. But for most Python 3 implementations the different **threads** do not actually execute at the same time: they merely appear to.\n",
    "(https://realpython.com/intro-to-python-threading)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synchronous programming"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **synchronous program* is executed one step at a time. Even with conditional branching, loops and function calls, you can still think about the code in terms of taking one execution step at a time. When each step is complete, the program moves on to the next one.\n",
    "(https://realpython.com/python-async-features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ansynchronous programming"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An asynchronous program behaves differently. It still takes one execution step at the time. The difference is that the system may not wait for an execution step to be completed before moving on to the next one.\n",
    "(https://realpython.com/python-async-features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synchronous version of getting response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "import requests\n",
    "\n",
    "def sync_version(urls):\n",
    "    for url in urls:\n",
    "        r = requests.get(f\"http://127.0.0.1:8000/items/{url}\")\n",
    "        print(r.json())\n",
    "\n",
    "start = perf_counter()\n",
    "sync_version(range(1, 2500))\n",
    "stop = perf_counter()\n",
    "print(\"time take:\", stop - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(1, 40)\n"
     ]
    }
   ],
   "source": [
    "print(range(1,40))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threading verios of getting response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = perf_counter()\n",
    "urls = range(1, 2500)\n",
    "\n",
    "def get_data(url):\n",
    "    r = request.get(url)\n",
    "    print(r.json())\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    executor.map(get_data, urls)\n",
    "\n",
    "stop = perf_counter()\n",
    "\n",
    "print(\"time taken:\", stop - start)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asynchronous version of getting response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch(s, url):\n",
    "    async with s.get(url) as r:\n",
    "        if r.status != 200:\n",
    "            r.raise_for_status()\n",
    "        return await r.text()\n",
    "    \n",
    "async def fetch_all(s, urls):\n",
    "    tasks = []\n",
    "    for url in urls:\n",
    "        task = asyncio.create_task(fetch(s, url))\n",
    "        tasks.append(task)\n",
    "    res = await asyncio.gather(*tasks)\n",
    "    return res\n",
    "\n",
    "async def main():\n",
    "    urls = range(1, 2500)\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        htmls = await fetch_all(session, urls)\n",
    "        print(htmls)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start = perf_counter()\n",
    "    asyncio.run(main())\n",
    "    stop = perf_counter()\n",
    "    print(\"time taken:\", stop - start)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Async to Sync code Example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sync code example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use **httpx** instead of standard **requests** because in **httpx** we have **AsyncClient** aviable.\n",
    "\n",
    "Couple things which are important to know:\n",
    "    * To actualy make use of async request we need to have a list of url beforehand. Either know them or we want to construct them by pulling them of the page.\n",
    "\n",
    "We want to transpose code below to our orginal code. To make it a little bit quicker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import asyncio\n",
    "\n",
    "async def get_data(client, url):\n",
    "    resp = await client.get(url)\n",
    "    return resp.json()['name']\n",
    "\n",
    "async def main():\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        tasks = []\n",
    "        for i in range(1, 150):\n",
    "            tasks.append(get_data(client, f\"https://rickandmortyapi.com/api/character/{i}\"))\n",
    "\n",
    "        characters = await asyncio.gather(*tasks)\n",
    "        for c in characters:\n",
    "            print(c)\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orginal code example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code goes through each page and gets all of the links for each product, vists that link and pulls out product information. What we gonna do is asyncing the product detals page part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "from selectolax.parser import HTMLParser\n",
    "from dataclasses import dataclass\n",
    "from rich import print\n",
    "\n",
    "@dataclass\n",
    "class Book:\n",
    "    title: str\n",
    "    UPC: str\n",
    "    product_type: str\n",
    "    price_inc_tax: str\n",
    "    price_exc_tax: str\n",
    "    tax: str\n",
    "    availability: str\n",
    "    num_of_reviews: str\n",
    "\n",
    "@dataclass\n",
    "class Response:\n",
    "    body_html: HTMLParser\n",
    "    next_page: dict\n",
    "\n",
    "def get_page(client, url):\n",
    "    \"\"\"\n",
    "    Fetches the HTML page using the specified client and URL.\n",
    "\n",
    "    Args:\n",
    "        client (httpx.Client): The HTTP client.\n",
    "        url (str): The URL to fetch.\n",
    "\n",
    "    Returns:\n",
    "        Response: An instance of the Response class containing the parsed HTML and next page information.\n",
    "    \"\"\"\n",
    "    resp = client.get(url)\n",
    "    data = HTMLParser(resp.text)\n",
    "    if data.css_first(\"li.next\"):\n",
    "        next_page = data.css_first(\"li.next a\").attributes\n",
    "    else:\n",
    "        next_page = {\"href\": None}\n",
    "    return Response(body_hmtl=data, next_page=next_page)\n",
    "\n",
    "def parse_link(html):\n",
    "    \"\"\"\n",
    "    Parses the HTML to extract detail page links.\n",
    "\n",
    "    Args:\n",
    "        html (HTMLParser): The parsed HTML\n",
    "\n",
    "    Returns:\n",
    "        list: A list of detail page links.\n",
    "    \"\"\"\n",
    "    links = html.css(\"article.product_pod h3 a\")\n",
    "    return[link.attrs[\"href\"] for link in links]\n",
    "\n",
    "def parse_detail(html, selector, index):\n",
    "    \"\"\"\n",
    "    Parses the HTML to extract a specific detail value.\n",
    "\n",
    "    Args: \n",
    "        html (HTMLParser): The parsed HTML.\n",
    "        selector (str): The CSS selector\n",
    "        index (int): The index of the element to extract.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted detail value or \"none\" if not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        value = html.css(selector)[index].text(strip=True)\n",
    "        return value\n",
    "    except:\n",
    "        return \"none\"\n",
    "    \n",
    "def detail_page_new(html):\n",
    "    \"\"\"\n",
    "    Parses the detail page HTML to create a Book object.\n",
    "\n",
    "    Args:\n",
    "        html (HTMLParser): The parsed detail page HTML.\n",
    "\n",
    "    Returns:\n",
    "        Book: An instance of the Book class representing the book details.\n",
    "    \"\"\"\n",
    "    new_book = Book(\n",
    "        title=parse_detail(html, \"h1\", 0),\n",
    "        UPC=parse_detail(html, \"table tbody tr td\", 0),\n",
    "        product_type=parse_detail(html, \"table tbody tr td\", 1),\n",
    "        price_inc_tax=parse_detail(html, \"table tbody tr td\", 2),\n",
    "        price_exc_tax=parse_detail(html, \"table tbody tr td\", 3),\n",
    "        tax=parse_detail(html, \"table tbody tr td\", 4),\n",
    "        availability=parse_detail(html, \"table tbody tr td\", 5),\n",
    "        num_of_reviews=parse_detail(html, \"table tbody tr td\", 6)\n",
    "    )\n",
    "    return new_book\n",
    "\n",
    "def check_url_text(value):\n",
    "    if \"catalogue\" not in value:\n",
    "        return \"catalogue/\" + value\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "def main():\n",
    "    results = []\n",
    "    base_url = \"https://books.toscrape.com/\"\n",
    "    url = \"https://books.toscrape.com/\"\n",
    "    client = httpx.Client()\n",
    "    while True:\n",
    "        data = get_page(client, url)\n",
    "        print(data)\n",
    "        detail_links = parse_links(data.body_html)\n",
    "        links = [base_url + check_url_text(link) for link in detail_links]\n",
    "        for link in detail_links:\n",
    "            product_page_data = get_page(client, base_url + check_url_text(link))\n",
    "            book_item = detail_page_new(product_page_data.body_html)\n",
    "            results.append(book_item)\n",
    "            print(book_item)\n",
    "        if data.next_page[\"href\"] == None:\n",
    "            client.close()\n",
    "            break\n",
    "        next_page_url = check_url_text(data.next_page[\"href\"])\n",
    "        url = base_url + str(next_page_url)\n",
    "    print(results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changed code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "from selectolax.parser import HTMLParser\n",
    "from dataclasses import dataclass\n",
    "from rich import print\n",
    "import asyncio\n",
    "\n",
    "@dataclass\n",
    "class Book:\n",
    "    title: str\n",
    "    UPC: str\n",
    "    product_type: str\n",
    "    price_inc_tax: str\n",
    "    price_exc_tax: str\n",
    "    tax: str\n",
    "    availability: str\n",
    "    num_of_reviews: str\n",
    "\n",
    "@dataclass\n",
    "class Response:\n",
    "    body_html: HTMLParser\n",
    "    next_page: dict\n",
    "\n",
    "def get_page(client, url):\n",
    "    \"\"\"\n",
    "    Fetches the HTML page using the specified client and URL.\n",
    "\n",
    "    Args:\n",
    "        client (httpx.Client): The HTTP client\n",
    "        url (str): The URL to fetch\n",
    "\n",
    "    Returns:\n",
    "        Response: An instance of the Response class containing the parsed HTML and next page information.\n",
    "    \"\"\"\n",
    "    resp = client.get(url)\n",
    "    data = HTMLParser(resp.text)\n",
    "    if data.css_first(\"li.next\"):\n",
    "        next_page = data.css_first(\"li.next a\").attributes\n",
    "    else:\n",
    "        next_page = {\"href\": None}\n",
    "    return Response(body_hmtl=data, next_page=next_page)\n",
    "\n",
    "def parse_link(html):\n",
    "    \"\"\"\n",
    "    Parses the HTML to extract detail page links.\n",
    "\n",
    "    Args:\n",
    "        html (HTMLParser): The parsed HTML.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of detail page links.\n",
    "    \"\"\"\n",
    "    links = html.css(\"article.product_pod h3 a\")\n",
    "    return[link.attrs[\"href\"] for link in links]\n",
    "\n",
    "def parse_detail(html, selector, index):\n",
    "    \"\"\"\n",
    "    Parses the HTML to extract a specific detail value.\n",
    "\n",
    "    Args:\n",
    "        html (HTMLParser): The parsed HTML.\n",
    "        selector (str): The CSS selector.\n",
    "        index (int): The index of the element to extract.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted detail value or \"none\" if not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        value = html.css(selector)[index].text(strip=True)\n",
    "        return value\n",
    "    except:\n",
    "        return \"none\"\n",
    "    \n",
    "def detail_page_new(html):\n",
    "    \"\"\"\n",
    "    Parses the detail page HTML to create a Book object.\n",
    "\n",
    "    Args:\n",
    "        html (HTMLParser): The parsed detail page HTML.\n",
    "    \n",
    "    Returns:\n",
    "        Book: An instance of the Book class representing the book details.\n",
    "    \"\"\"\n",
    "    new_book = Book(\n",
    "        title=parse_detail(html, \"h1\", 0),\n",
    "        UPC=parse_detail(html, \"table tbody tr td\", 0),\n",
    "        product_type=parse_detail(html, \"table tbody tr td\", 1),\n",
    "        price_inc_tax=parse_detail(html, \"table tbody tr td\", 2),\n",
    "        price_exc_tax=parse_detail(html, \"table tbody tr td\", 3),\n",
    "        tax=parse_detail(html, \"table tbody tr td\", 4),\n",
    "        availability=parse_detail(html, \"table tbody tr td\", 5),\n",
    "        num_of_reviews=parse_detail(html, \"table tbody tr td\", 6)\n",
    "    )\n",
    "    return new_book\n",
    "\n",
    "def check_url_text(value):\n",
    "    if \"catalogue\" not in value:\n",
    "        return \"catalogue/\" + value\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "async def async_get_data(client, url):\n",
    "    \"\"\"\n",
    "    Asynchronously fetches dat` from the specified URL using the client.\n",
    "\n",
    "    Args:\n",
    "        client (httpx.AsyncClient): The asynchronous HTTP client.\n",
    "        url (str): The URL to fetch.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    resp = await client.get(url)\n",
    "    html = HTMLParser(resp.text)\n",
    "    print(detail_page_new(html))\n",
    "\n",
    "async def with_async(links):\n",
    "    \"\"\"\n",
    "    Executes the async_get_data coroutine for multiple links in parallel.\n",
    "\n",
    "    Args:\n",
    "        links (list): A list of URLs to fetch.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        tasks = []\n",
    "        for link in links:\n",
    "            tasks.append(async_get_data(client, link))\n",
    "        return await asyncio.gather(*tasks)\n",
    "\n",
    "def main():\n",
    "    results = []\n",
    "    base_url = \"https://books.toscrape.com/\"\n",
    "    url = \"https://books.toscrape.com/\"\n",
    "    client = httpx.Client()\n",
    "    while True:\n",
    "        data = get_page(client, url)\n",
    "        print(data)\n",
    "        detail_links = parse_links(data.body_html)\n",
    "        links = [base_url + check_url_text(link) for link in detail_links]\n",
    "        asyncio.run(with_async(links))\n",
    "        # for link in detail_links:\n",
    "        #     product_page_data = get_page(client, base_url + check_url_text(link))\n",
    "        #     book_item = detail_page_new(product_page_data.body_html)\n",
    "        #     results.append(book_item)\n",
    "        #     print(book_item)\n",
    "        if data.next_page[\"href\"] == None:\n",
    "            client.close()\n",
    "            break\n",
    "        next_page_url = check_url_text(data.next_page[\"href\"])\n",
    "        url = base_url + str(next_page_url)\n",
    "    print(results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Async with pagination"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "from selectolax.parser import HTMLParser\n",
    "import asyncio\n",
    "from dataclasses import dataclass\n",
    "from itertools import chain\n",
    "\n",
    "@dataclass\n",
    "class Book:\n",
    "    title: str\n",
    "    UPC: str\n",
    "    product_type: str\n",
    "    price_inc_tax: str\n",
    "    price_exc_tax: str\n",
    "    tax: str\n",
    "    availability: str\n",
    "    num_of_reviews: str\n",
    "\n",
    "@dataclass\n",
    "class Response:\n",
    "    body_html: HTMLParser\n",
    "    next_page: dict\n",
    "\n",
    "def is_integer(val):\n",
    "    try:\n",
    "        return int(val)\n",
    "    except ValueError:\n",
    "        return\n",
    "\n",
    "def parse_detail(html, selector, index):\n",
    "    \"\"\"\n",
    "    Parses the HTML to extract a specific detail value.\n",
    "\n",
    "    Args:\n",
    "        html (HTMLParser): The parsed HTML.\n",
    "        selector (str): The CSS selector.\n",
    "        index (int): The index of the element to extract.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted detail value or \"none\" if not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        value = html.css(selector)[index].text(strip=True)\n",
    "        return value\n",
    "    except:\n",
    "        return \"none\"\n",
    "    \n",
    "def detail_page_new(html):\n",
    "    \"\"\"\n",
    "    Parses the detail page HTML to create a Book object.\n",
    "\n",
    "    Args:\n",
    "        html (HTMLParser): The parsed detail page HTML.\n",
    "\n",
    "    Returns:\n",
    "        Book: An instance of the Book class representing the book details.\n",
    "    \"\"\"\n",
    "    new_book = Book(\n",
    "        title=parse_detail(html, \"h1\", 0),\n",
    "        UPC=parse_detail(html, \"table tbody tr td\", 0),\n",
    "        product_type=parse_detail(html, \"table tbody tr td\", 1),\n",
    "        price_inc_tax=parse_detail(html, \"table tbody tr td\", 2),\n",
    "        price_exc_tax=parse_detail(html, \"table tbody tr td\", 3),\n",
    "        tax=parse_detail(html, \"table tbody tr td\", 4),\n",
    "        availability=parse_detail(html, \"table tbody tr td\", 5),\n",
    "        num_of_reviews=parse_detail(html, \"table tbody tr td\", 6)\n",
    "    )\n",
    "    return new_book\n",
    "\n",
    "def get_total_pages(url):\n",
    "    \"\"\"\n",
    "    Fetches the total number of pages from the website.\n",
    "\n",
    "    Args:\n",
    "        url (str): The base URL of the website.\n",
    "    \n",
    "    Returns:\n",
    "        int: The total number of pages.\n",
    "    \"\"\"\n",
    "    resp = httpx.get(url)\n",
    "    html = HTMLParser(resp.text)\n",
    "    pages = html.css_first(\"ul.pager li.current\").text(strip=True).split()\n",
    "    pages_int = [is_integer(page) for page in pages if is_integer(page) is not None]\n",
    "    last_page = max(pages_int)\n",
    "    return last_page\n",
    "\n",
    "def parse_links(html):\n",
    "    \"\"\"\n",
    "    Parses the HTML to extract detail page links.\n",
    "\n",
    "    Args:\n",
    "        html (HTMLParser): The parsed HTML.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of detail page links.\n",
    "    \"\"\"\n",
    "    links = html.css(\"article.product_pod h3 a\")\n",
    "    return [link.attrs[\"href\"] for link in links]\n",
    "\n",
    "async def get_async_links(client, url):\n",
    "    \"\"\"\n",
    "    Asynchronously fetches detail page links from the specified URL using the client.\n",
    "\n",
    "    Args:\n",
    "        client (httpx.AsyncClient): The asynchronous HTTP client.\n",
    "        url (str): The URL to fetch.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of detail page links.\n",
    "    \"\"\"\n",
    "    resp = await client.get(url)\n",
    "    html = HTMLParser(resp.text)\n",
    "    links = parse_links(html)\n",
    "    return links\n",
    "\n",
    "async def get_links():\n",
    "    \"\"\"\n",
    "    Asynchronously fetches detail page links from multiple pages.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of lists, containing detail page links for each page.\n",
    "    \"\"\"\n",
    "    base_url = \"https://books.toscrape.com/catalogue/\"\n",
    "    async with htppx.AsyncClient() as client:\n",
    "        tasks = []\n",
    "        for i in range(1, get_total_pages(base_url + \"page-1.html\" + 1)):\n",
    "            tasks.append(\n",
    "                asyncio.ensure_future(\n",
    "                    get_async_links(client, base_url + f\"page-{i}.html\")\n",
    "                )\n",
    "            )\n",
    "\n",
    "async def get_async_details(client, url):\n",
    "    \"\"\"\n",
    "    Asynchronously fetches book details from the specified URL using the client.\n",
    "\n",
    "    Args:\n",
    "        client (httpx.AsyncClient): The asynchronous HTTP client.\n",
    "        url (str): The URL to fetch.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    resp = await client.get(url)\n",
    "    html = HTMLParser(resp.text)\n",
    "    print(detail_page_new(html))\n",
    "\n",
    "async def get_detail(urls):\n",
    "    \"\"\"\n",
    "    Asynchronously fetches book details for a list of URLs.\n",
    "\n",
    "    Args:\n",
    "        urls (list): A list of URLs to fetch.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    base_url = \"https://books.toscrape.com/catalogue/\"\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        tasks = []\n",
    "        for url in urls:\n",
    "            tasks.append(\n",
    "                asyncio.ensure_future(get_async_details(client, base_url + url))\n",
    "            )\n",
    "\n",
    "def main():\n",
    "    links = asyncio.run(get_links())\n",
    "    detail_links = []\n",
    "    for link in chain(links):\n",
    "        for l in chain(link):\n",
    "            detail_links.append(l)\n",
    "    details = asyncio.run(get_detail(detail_links))\n",
    "    print(len(details))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
